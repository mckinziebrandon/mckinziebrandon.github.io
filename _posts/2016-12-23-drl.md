---
layout: post
title:  "Condensed Tutorials 2 - Deep Reinforcement Learning"
date:   2016-12-23
excerpt: "Part I from the *Demystifying Deep Reinforcement Learning* post at nervanasys."
tags:
- condensed tutorial
- deep learning
- reinforcement learning
comments: false
---

[Link to tutorial](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/).


__Reinforcement learning__. vulnerable to the *credit assignment problem* - i.e. unsure which of the preceding actions was responsible for getting some reward and to what extent. Also need to address the famous *explore-exploit dilemma* when deciding what strategies to use. 

---------------------------------------------------

__Markov Decision Process__. Most common method for representing a reinforcement problem. MDPs consist of states, actions, and rewards. Total reward is sum of current (includes previous) and *discounted* future rewards:
\\[ 
    R_t = r_t \gamma(r_{t + 1}	 + \gamma(r_{t + 2} + \ldots)) = r_t + \gamma R_{t + 1} 
\\]

---------------------------------------------------

__Q - learning__. Define function \\(Q(s, a)\\) to be best possible score at end of game after performing action \\(a\\) in state \\(s\\); the "quality" of an action from a given state. The recursive definition of Q (for one transition) is given below in the _Bellman equation_.

\\[ 
    Q(s, a) = r + \gamma \mathrm{max}_{a'} Q(s', a') 
\\]

and updates are computed with a learning rate \\(\alpha\\) as

\\[ 
    Q(s_t, a_t) = 
    (1 - \alpha) \cdot Q(s_{t -1}, a_{t - 1}) 
    + \alpha \cdot (r + \gamma \max_{a\prime} Q(s\prime_{t + 1}, a\prime_{t+1}))
\\]

---------------------------------------------------
 

 __Deep Q Network__. Deep learning can take deal with issues related to prohibitively large state spaces. The implementation chosen by DeepMind was to represent the Q-function with a neural network, with the states (pixels) as the input and Q-values as output, where the number of output neurons is the number of possible actions from the input state. We can optimize with simple squared loss:
 
<img src="{{site.url}}/assets/img/drl/DRL_loss.PNG" style="width: 250px;"/>

and our algorithm from some state \\(s\\) becomes
1. __First forward pass__ from \\(s\\) to get all predicted Q-values for each possible action. Choose action corresponding to max output, leading to next \\(s'\\).

2. __Second forward pass__ from \\(s'\\) and again compute \\(\max_{a'} Q(s', a')\\). 

3. __Set target output__ for each action \\(a'\\) from \\(s'\\). For the action corresponding to max (from step 2) set its target as \\(r + \gamma \max_{a'} Q(s', a')\\), and for all other actions set target to same as originally returned from step 1, making the error 0 for those outputs. (Interpret as update to our guess for the best Q-value, and keep the others the same.)
	
4. __Update weights__ using backprop. 


---------------------------------------------------


__Experience Replay__. This the most important trick for helping convergence of Q-values when approximating with non-linear functions. During gameplay all the experience \\(<s, a, r, s\prime>\\) are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition. 


---------------------------------------------------


__Exploration__. One could say that initializing the Q-values randomly and then picking the max is essentially a form of exploitation. However, this type of exploration is *greedy*, which can be tamed/fixed with __\\(\varepsilon\\)-greedy exploration__. This incorporates a degree of randomness when choosing next action at *all* time-steps, determined by probability \\(\varepsilon\\) that we choose the next action randomly. For example, DeepMind decreases \\(\varepsilon\\) over time from 1 to 0.1. 


__Deep Q-Learning Algorithm__.

<img src="{{site.url}}/assets/img/drl/DRL_alg.PNG" style="width: 400px;"/>


