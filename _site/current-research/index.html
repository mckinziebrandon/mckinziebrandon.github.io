<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]--><!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8"><![endif]--><!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9"><![endif]--><!--[if gt IE 8]><!--><html class="no-js">
<!--<![endif]--> <head> <meta charset="UTF-8"> <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"> <title>Automatic Architecture Generation for Deep Neural Networks – Brandon McKinzie</title> <meta name="description" content="A bit of physics here, and a bit of computer science there."> <meta name="keywords" content="research, deep learning, python, tensorflow"> <!-- Twitter Cards --> <meta name="twitter:card" content="summary"> <meta name="twitter:image" content="http://localhost:4000/assets/img/Me.jpg"> <meta name="twitter:title" content="Automatic Architecture Generation for Deep Neural Networks"> <meta name="twitter:description" content="A brief overview of my current research."> <!-- Open Graph --> <meta property="og:locale" content="en_US"> <meta property="og:type" content="article"> <meta property="og:title" content="Automatic Architecture Generation for Deep Neural Networks"> <meta property="og:description" content="A brief overview of my current research."> <meta property="og:url" content="http://localhost:4000/current-research/"> <meta property="og:site_name" content="Brandon McKinzie"> <meta property="og:image" content="http://localhost:4000/assets/img/Me.jpg"> <link rel="canonical" href="http://localhost:4000/current-research/"> <link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Brandon McKinzie Feed"> <!-- Handheld --> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <!-- CSS --> <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css"> <!-- JS --> <script src="http://localhost:4000/assets/js/modernizr-3.3.1.custom.min.js"></script> <!-- Favicons --> <link rel="apple-touch-icon" href="http://localhost:4000/assets/img/favicons/apple-icon-precomposed.png"> <link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000/assets/img/favicons/apple-icon-72x72.png"> <link rel="apple-touch-icon" sizes="114x114" href="http://localhost:4000/assets/img/favicons/apple-icon-114x114.png"> <link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000/assets/img/favicons/apple-icon-144x144.png"> <link rel="shortcut icon" type="image/png" href="http://localhost:4000/favicon.png"> <link rel="shortcut icon" href="http://localhost:4000/favicon.ico"> <!-- Background Image --> <style type="text/css">body {background-image:url(http://i.imgur.com/oRIhuWM.jpg); background-repeat: no-repeat; background-size: cover; }</style> <!-- Post Feature Image --> </head> <body> <nav id="dl-menu" class="dl-menuwrapper" role="navigation"> <button class="dl-trigger">Open Menu</button> <ul class="dl-menu"> <li><a href="http://localhost:4000/">Home</a></li> <li> <a href="#">About</a> <ul class="dl-submenu"> <li> <img src="http://localhost:4000/assets/img/Me.jpg" alt="Brandon McKinzie photo" class="author-photo"> <h4>Brandon McKinzie</h4> <p>A bit of physics here, and a bit of computer science there.</p> </li> <li><a href="http://localhost:4000/about/"><span class="btn btn-inverse">Learn More</span></a></li> <li> <a href="mailto:mckinziebrandon@berkeley.edu" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-envelope-square"></i> Email</a> </li> <li> <a href="http://linkedin.com/in/brandon-mckinzie-452a03112" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a> </li> <li> <a href="http://github.com/mckinziebrandon" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-github"></i> Github</a> </li> </ul>
<!-- /.dl-submenu --> </li> <li> <a href="#">Posts</a> <ul class="dl-submenu"> <li><a href="http://localhost:4000/posts/">All Posts</a></li> <li><a href="http://localhost:4000/tags/">All Tags</a></li> </ul> </li> <li><a href="http://localhost:4000/research/">Research</a></li> <li><a href="http://localhost:4000/notes/">Notes</a></li> <li><a href="http://mckinziebrandon.me/TensorflowNotebooks" target="_blank" rel="noopener noreferrer">TensorFlow Notebooks</a></li> </ul>
<!-- /.dl-menu --> </nav><!-- /.dl-menuwrapper --> <!-- Header --> <header class="header" role="banner"> <div class="wrapper animated fadeIn"> <div class="content"> <div class="post-title "> <h1>Automatic Architecture Generation for Deep Neural Networks</h1> <h4>27 Nov 2016</h4> <p class="reading-time"> <i class="fa fa-clock-o"></i> Reading time ~7 minutes </p>
<!-- /.entry-reading-time --> <a class="btn zoombtn" href="http://localhost:4000/research/"> <i class="fa fa-chevron-left"></i> </a> </div> <h1 id="motivation">Motivation</h1> <p>One of the challenges in building an effective deep neural network is, of course, figuring out how to build it! There are an infinite number of possible combinations for hyperparameters such as . . .</p> <ul> <li>number of layers (the “depth”)</li> <li>number of parallel layers (the “width”)</li> <li>activation functions for each (computational) layer</li> <li>learning rate(s) and/or momentum</li> </ul> <p>. . . and the list goes on and on. So how do the experts do it? You may be surprised to find out that machine learning is still a lot of trial-and-error. However, this just doesn’t feel right. Surely there is a way we can automate this process.</p> <p>My current research, led by Professor Dawn Song’s research group at UC Berkeley, aims to solve this problem. Rather than having humans tinker around with designing deep neural networks, this project aims to write software that can learn to build deep networks on its own.</p> <h1 id="early-stopping">Early Stopping</h1> <p>Say you’re a sentient machine trying to figure out the best network build. As we’ve discussed, you have about an infinite number of ways to approach this. Regardless of what you choose, you’ll certainly need a way to <em>evaluate</em> how well you’ve done after constructing a given architecture. Unfortunately, even for a fast AI such as yourself, training and evaluation can take a <em>really</em> long time.</p> <p>A common technique for speeding up the training and evaluation, known as <strong>early stopping</strong>, involving periodically “peeking under the hood” of the network as it learns, and determining whether or not (a) it is mostly done learning (negligible improvement), or (b) it has “learned enough,” where it is up to the designer to determine what “enough” means.</p> <p>So how does one implement early stopping in practice? Surprisingly, the support for this (in tensorflow/tflearn) is slim. If you try digging around on how to use tensorflow for this, you’ll likely find code snippets such as the following.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">validation_metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s">"accuracy"</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">streaming_accuracy</span><span class="p">,</span>
                      <span class="s">"precision"</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">streaming_precision</span><span class="p">,</span>
                      <span class="s">"recall"</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">streaming_recall</span><span class="p">}</span>

<span class="n">validation_monitor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">monitors</span><span class="o">.</span><span class="n">ValidationMonitor</span><span class="p">(</span>
    <span class="n">test_set</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
    <span class="n">test_set</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
    <span class="n">every_n_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="c">#metrics=validation_metrics,</span>
    <span class="n">early_stopping_metric</span><span class="o">=</span><span class="s">'loss'</span><span class="p">,</span>
    <span class="n">early_stopping_metric_minimize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">training_set</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
               <span class="n">y</span><span class="o">=</span><span class="n">training_set</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
               <span class="n">steps</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
               <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="n">validation_monitor</span><span class="p">])</span></code></pre></figure> <p>Unfortunately, a lot of these recommended approaches are outdated. Although I did manage to hack together a working implementation of early stopping with tf.contrib.learn (see TensorFlow Notebooks section), I had to suppress a lot of warning/info messages from TensorFlow (again, <em>a lot</em>) that were solely due to me using certain methods of theirs (as recommended by their tutorials) that are now deprecated and confusingly don’t seem to have direct replacements in new versions. But enough of that for now, let’s see how we can use the tflearn library instead for a more reliable solution.</p> <h2 id="a-simple-solution">A Simple Solution</h2> <p>The cleanest way I’ve implemented early stopping thus far has been with a little help from tflearn (distinct from tf.contrib.learn). Although they currently don’t support early stopping explicitly, a nice workaround is defining a <strong>callback</strong> object. Here is a working proof-of-concept example below. But first, a brief overview.</p> <p>The following is a code snippet directly from <a href="https://github.com/tflearn/tflearn/blob/master/tflearn/helpers/trainer.py#L281">trainer.py</a> in the tflearn github repository, where I’m only showing the relevant parts/logic.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">try</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epoch</span><span class="p">):</span>
        <span class="c"># . . . Setup stuff for epoch here . . . </span>
        <span class="k">for</span> <span class="n">batch_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_batches_len</span><span class="p">):</span>
            <span class="c"># . . . Setup stuff for next batch here . . . </span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">train_op</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_ops</span><span class="p">):</span>
                <span class="n">caller</span><span class="o">.</span><span class="n">on_sub_batch_begin</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_state</span><span class="p">)</span>
                
                <span class="c"># Train our model and store desired information in the train_op that</span>
                <span class="c"># we (the user) pass to the trainer as an initialization argument.</span>
                <span class="n">snapshot</span> <span class="o">=</span> <span class="n">train_op</span><span class="o">.</span><span class="n">_train</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_state</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
                                           <span class="p">(</span><span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_checkpoint_path</span><span class="p">)</span> <span class="o">|</span> <span class="n">snapshot_epoch</span><span class="p">),</span>
                                           <span class="n">snapshot_step</span><span class="p">,</span>
                                           <span class="n">show_metric</span><span class="p">)</span>
                                           
                <span class="c"># Update training state. The training state object tells us </span>
                <span class="c"># how our model is doing at various stages of training.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">train_op</span><span class="p">,</span> <span class="n">train_ops_count</span><span class="p">)</span>

            <span class="c"># All optimizers batch end</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">incr_global_step</span><span class="p">)</span>
            <span class="n">caller</span><span class="o">.</span><span class="n">on_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_state</span><span class="p">,</span> <span class="n">snapshot</span><span class="p">)</span>

        <span class="c"># ---------- [What we care about] -------------</span>
        <span class="c"># Epoch end. We define what on_epoch_end does. In this</span>
        <span class="c"># case, I'll have it raise an exception if our validation accuracy</span>
        <span class="c"># reaches some desired threshold. </span>
        <span class="n">caller</span><span class="o">.</span><span class="n">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_state</span><span class="p">)</span>
        <span class="c"># ---------------------------------------------</span>

<span class="k">finally</span><span class="p">:</span>
    <span class="c"># Once we raise the exception, this code block will execute. </span>
    <span class="c"># Note only afterward will our catch block execute. </span>
    <span class="n">caller</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_state</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_ops</span><span class="p">:</span>
        <span class="n">t</span><span class="o">.</span><span class="n">train_dflow</span><span class="o">.</span><span class="n">interrupt</span><span class="p">()</span>
    <span class="c"># Set back train_ops</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">train_ops</span> <span class="o">=</span> <span class="n">original_train_ops</span></code></pre></figure> <h2 id="setup-the-basic-network-architecture">Setup the Basic Network Architecture</h2> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tflearn</span>
<span class="kn">import</span> <span class="nn">tflearn.datasets.mnist</span> <span class="kn">as</span> <span class="nn">mnist</span>

<span class="n">trainX</span><span class="p">,</span> <span class="n">trainY</span><span class="p">,</span> <span class="n">testX</span><span class="p">,</span> <span class="n">testY</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">n_features</span> <span class="o">=</span> <span class="mi">784</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c"># Define the inputs/outputs/weights as usual.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s">"float"</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_features</span><span class="p">])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s">"float"</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">])</span>

<span class="c"># Define the connections/weights and biases between layers.</span>
<span class="n">W1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s">'W1'</span><span class="p">)</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s">'W2'</span><span class="p">)</span>
<span class="n">W3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s">'W3'</span><span class="p">)</span>

<span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s">'b1'</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s">'b2'</span><span class="p">)</span>
<span class="n">b3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_classes</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s">'b3'</span><span class="p">)</span>

<span class="c"># Define the operations throughout the network.</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W1</span><span class="p">),</span> <span class="n">b1</span><span class="p">))</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">W2</span><span class="p">),</span> <span class="n">b2</span><span class="p">))</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">W3</span><span class="p">),</span> <span class="n">b3</span><span class="p">)</span>


<span class="c"># Define the optimization problem.</span>
<span class="n">loss</span>      <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">accuracy</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'acc'</span><span class="p">)</span>
</code></pre></div> <div class="highlighter-rouge"><pre class="highlight"><code>hdf5 not supported (please install/reinstall h5py)
Extracting mnist/train-images-idx3-ubyte.gz
Extracting mnist/train-labels-idx1-ubyte.gz
Extracting mnist/t10k-images-idx3-ubyte.gz
Extracting mnist/t10k-labels-idx1-ubyte.gz
</code></pre></div> <h2 id="define-the-trainop-and-trainer-objects">Define the TrainOp and Trainer Objects</h2> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">trainop</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">TrainOp</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">train_ops</span><span class="o">=</span><span class="n">trainop</span><span class="p">,</span> <span class="n">tensorboard_verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div> <h1 id="the-earlystoppingcallback-class">The EarlyStoppingCallback Class</h1> <p>I show a proof-of-concept version of early stopping below. This is the simplest possible case: just stop training after the first epoch no matter what. It is up to the user to decide the conditions they want to trigger the stopping on.</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EarlyStoppingCallback</span><span class="p">(</span><span class="n">tflearn</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_acc_thresh</span><span class="p">):</span>
        <span class="s">""" Note: We are free to define our init function however we please. """</span>
        <span class="c"># Store a validation accuracy threshold, which we can compare against</span>
        <span class="c"># the current validation accuracy at, say, each epoch, each batch step, etc.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val_acc_thresh</span> <span class="o">=</span> <span class="n">val_acc_thresh</span>
    
    <span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_state</span><span class="p">):</span>
        <span class="s">""" 
        This is the final method called in trainer.py in the epoch loop. 
        We can stop training and leave without losing any information with a simple exception.  
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Terminating training at the end of epoch"</span><span class="p">,</span> <span class="n">training_state</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>
        <span class="k">raise</span> <span class="nb">StopIteration</span>
    
    <span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_state</span><span class="p">):</span>
        <span class="s">"""
        Furthermore, tflearn will then immediately call this method after we terminate training, 
        (or when training ends regardless). This would be a good time to store any additional 
        information that tflearn doesn't store already.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Successfully left training! Final model accuracy:"</span><span class="p">,</span> <span class="n">training_state</span><span class="o">.</span><span class="n">acc_value</span><span class="p">)</span>
       
        
<span class="c"># Initialize our callback with desired accuracy threshold.  </span>
<span class="n">early_stopping_cb</span> <span class="o">=</span> <span class="n">EarlyStoppingCallback</span><span class="p">(</span><span class="n">val_acc_thresh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div> <h1 id="result-train-the-model-and-stop-early">Result: Train the Model and Stop Early</h1> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="c"># Give it to our trainer and let it fit the data. </span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">feed_dicts</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">trainX</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">trainY</span><span class="p">},</span> 
                <span class="n">val_feed_dicts</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">testX</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">testY</span><span class="p">},</span> 
                <span class="n">n_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                <span class="n">show_metric</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c"># Calculate accuracy and display at every step.</span>
                <span class="n">callbacks</span><span class="o">=</span><span class="n">early_stopping_cb</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">StopIteration</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Caught callback exception. Returning control to user program."</span><span class="p">)</span>
    
</code></pre></div> <div class="highlighter-rouge"><pre class="highlight"><code>Training Step: 860  | total loss: [1m[32m1.73372[0m[0m
| Optimizer | epoch: 002 | loss: 1.73372 - acc: 0.8196 | val_loss: 1.87058 - val_acc: 0.8011 -- iter: 55000/55000
Training Step: 860  | total loss: [1m[32m1.73372[0m[0m
| Optimizer | epoch: 002 | loss: 1.73372 - acc: 0.8196 | val_loss: 1.87058 - val_acc: 0.8011 -- iter: 55000/55000
--
Terminating training at the end of epoch 2
Successfully left training! Final model accuracy: 0.8196054697036743
Caught callback exception. Returning control to user program.
</code></pre></div> <h1 id="appendix">Appendix</h1> <p>For my own reference, this is the code I started with before tinkering with the early stopping solution above.</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">absolute_import</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">urllib</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">scipy.io</span> <span class="kn">import</span> <span class="n">arff</span>

<span class="kn">import</span> <span class="nn">tflearn</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">shuffle</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">tflearn.data_utils</span> <span class="kn">import</span> <span class="n">shuffle</span><span class="p">,</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">tflearn.layers.core</span> <span class="kn">import</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">fully_connected</span>
<span class="kn">from</span> <span class="nn">tflearn.layers.conv</span> <span class="kn">import</span> <span class="n">conv_2d</span><span class="p">,</span> <span class="n">max_pool_2d</span>
<span class="kn">from</span> <span class="nn">tflearn.layers.normalization</span> <span class="kn">import</span> <span class="n">local_response_normalization</span><span class="p">,</span> <span class="n">batch_normalization</span>
<span class="kn">from</span> <span class="nn">tflearn.layers.estimator</span> <span class="kn">import</span> <span class="n">regression</span>
<span class="kn">import</span> <span class="nn">tflearn.datasets.mnist</span> <span class="kn">as</span> <span class="nn">mnist</span>


<span class="c"># Load the data and handle any preprocessing here.</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">testX</span><span class="p">,</span> <span class="n">testY</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span>  <span class="o">=</span> <span class="n">shuffle</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">X</span>     <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">testX</span> <span class="o">=</span> <span class="n">testX</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c"># Define our network architecture: a simple 2-layer network of the form</span>
<span class="c"># InputImages -&gt; Fully Connected -&gt; Softmax</span>
<span class="n">out_readin1</span>          <span class="o">=</span> <span class="n">input_data</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">out_fully_connected2</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">out_readin1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">out_softmax3</span>         <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">out_fully_connected2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)</span>

<span class="nb">hash</span><span class="o">=</span><span class="s">'f0c188c3777519fb93f1a825ca758a0c'</span>
<span class="n">scriptid</span><span class="o">=</span><span class="s">'MNIST-f0c188c3777519fb93f1a825ca758a0c'</span>

<span class="c"># Define our training metrics. </span>
<span class="n">network</span> <span class="o">=</span> <span class="n">regression</span><span class="p">(</span><span class="n">out_softmax3</span><span class="p">,</span> 
                     <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> 
                     <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> 
                     <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> 
                     <span class="n">name</span><span class="o">=</span><span class="s">'target'</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">DNN</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">tensorboard_verbose</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> 
          <span class="n">n_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
          <span class="n">validation_set</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testY</span><span class="p">),</span> 
          <span class="n">snapshot_step</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
          <span class="n">snapshot_epoch</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
          <span class="n">show_metric</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
          <span class="n">run_id</span><span class="o">=</span><span class="n">scriptid</span><span class="p">)</span>


<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>
<span class="n">auc</span><span class="o">=</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">testY</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">'macro'</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">accuracy</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span><span class="n">testY</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy:"</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"ROC AUC Score:"</span><span class="p">,</span> <span class="n">auc</span><span class="p">)</span>

</code></pre></div> <div class="entry-meta"> <br> <hr> <span class="entry-tags"><a href="http://localhost:4000/tags/#research" title="Pages tagged research" class="tag"><span class="term">research</span></a><a href="http://localhost:4000/tags/#deep%20learning" title="Pages tagged deep learning" class="tag"><span class="term">deep learning</span></a><a href="http://localhost:4000/tags/#python" title="Pages tagged python" class="tag"><span class="term">python</span></a><a href="http://localhost:4000/tags/#tensorflow" title="Pages tagged tensorflow" class="tag"><span class="term">tensorflow</span></a></span> <span class="social-share"> <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/current-research/" title="Share on Facebook" class="tag"> <span class="term"><i class="fa fa-facebook-square"></i> Like</span> </a> <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/current-research/" title="Share on Twitter" class="tag"> <span class="term"><i class="fa fa-twitter-square"></i> Tweet</span> </a> <a href="https://plus.google.com/share?url=http://localhost:4000/current-research/" title="Share on Google+" class="tag"> <span class="term"><i class="fa fa-google-plus-square"></i> +1</span> </a> </span> <div style="clear:both"></div> </div> </div> </div> </header> <!-- JS --> <script src="http://localhost:4000/assets/js/jquery-1.12.0.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.dlmenu.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.goup.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.magnific-popup.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.fitvid.min.js"></script> <script src="http://localhost:4000/assets/js/scripts.js"></script> <!-- MathJax --> <script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </body> </html>
