<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]--><!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8"><![endif]--><!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9"><![endif]--><!--[if gt IE 8]><!--><html class="no-js">
<!--<![endif]--> <head> <meta charset="UTF-8"> <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"> <title>Condensed Tutorials 2 - Deep Reinforcement Learning – Brandon McKinzie</title> <meta name="description" content="A bit of physics here, and a bit of computer science there."> <meta name="keywords" content="condensed tutorial, deep learning, reinforcement learning"> <!-- Twitter Cards --> <meta name="twitter:card" content="summary"> <meta name="twitter:image" content="http://localhost:4000/assets/img/Me.jpg"> <meta name="twitter:title" content="Condensed Tutorials 2 - Deep Reinforcement Learning"> <meta name="twitter:description" content="Part I from the Demystifying Deep Reinforcement Learning post at nervanasys."> <!-- Open Graph --> <meta property="og:locale" content="en_US"> <meta property="og:type" content="article"> <meta property="og:title" content="Condensed Tutorials 2 - Deep Reinforcement Learning"> <meta property="og:description" content="Part I from the Demystifying Deep Reinforcement Learning post at nervanasys."> <meta property="og:url" content="http://localhost:4000/drl/"> <meta property="og:site_name" content="Brandon McKinzie"> <meta property="og:image" content="http://localhost:4000/assets/img/Me.jpg"> <link rel="canonical" href="http://localhost:4000/drl/"> <link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Brandon McKinzie Feed"> <!-- Handheld --> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <!-- CSS --> <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css"> <!-- JS --> <script src="http://localhost:4000/assets/js/modernizr-3.3.1.custom.min.js"></script> <!-- Favicons --> <link rel="apple-touch-icon" href="http://localhost:4000/assets/img/favicons/apple-icon-precomposed.png"> <link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000/assets/img/favicons/apple-icon-72x72.png"> <link rel="apple-touch-icon" sizes="114x114" href="http://localhost:4000/assets/img/favicons/apple-icon-114x114.png"> <link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000/assets/img/favicons/apple-icon-144x144.png"> <link rel="shortcut icon" type="image/png" href="http://localhost:4000/favicon.png"> <link rel="shortcut icon" href="http://localhost:4000/favicon.ico"> <!-- Background Image --> <style type="text/css">body {background-image:url(http://i.imgur.com/oRIhuWM.jpg); background-repeat: no-repeat; background-size: cover; }</style> <!-- Post Feature Image --> </head> <body> <nav id="dl-menu" class="dl-menuwrapper" role="navigation"> <button class="dl-trigger">Open Menu</button> <ul class="dl-menu"> <li><a href="http://localhost:4000/">Home</a></li> <li> <a href="#">About</a> <ul class="dl-submenu"> <li> <img src="http://localhost:4000/assets/img/Me.jpg" alt="Brandon McKinzie photo" class="author-photo"> <h4>Brandon McKinzie</h4> <p>A bit of physics here, and a bit of computer science there.</p> </li> <li><a href="http://localhost:4000/about/"><span class="btn btn-inverse">Learn More</span></a></li> <li> <a href="mailto:mckinziebrandon@berkeley.edu" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-envelope-square"></i> Email</a> </li> <li> <a href="http://linkedin.com/in/brandon-mckinzie-452a03112" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a> </li> <li> <a href="http://github.com/mckinziebrandon" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-github"></i> Github</a> </li> </ul>
<!-- /.dl-submenu --> </li> <li> <a href="#">Posts</a> <ul class="dl-submenu"> <li><a href="http://localhost:4000/posts/">All Posts</a></li> <li><a href="http://localhost:4000/tags/">All Tags</a></li> </ul> </li> <li><a href="http://localhost:4000/research/">Research</a></li> <li><a href="http://localhost:4000/notes/">Notes</a></li> <li><a href="http://mckinziebrandon.me/TensorflowNotebooks" target="_blank" rel="noopener noreferrer">TensorFlow Notebooks</a></li> </ul>
<!-- /.dl-menu --> </nav><!-- /.dl-menuwrapper --> <!-- Header --> <header class="header" role="banner"> <div class="wrapper animated fadeIn"> <div class="content"> <div class="post-title "> <h1>Condensed Tutorials 2 - Deep Reinforcement Learning</h1> <h4>23 Dec 2016</h4> <p class="reading-time"> <i class="fa fa-clock-o"></i> Reading time ~2 minutes </p>
<!-- /.entry-reading-time --> <a class="btn zoombtn" href="http://localhost:4000/posts/"> <i class="fa fa-chevron-left"></i> </a> </div> <p><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Link to tutorial</a>.</p> <p><strong>reinforcement learning</strong>. vulnerable to the <em>credit assignment problem</em> - i.e. unsure which of the preceding actions was responsible for getting some reward and to what extent. Also need to address the famous <em>explore-exploit dilemma</em> when deciding what strategies to use.</p> <p><strong>Markov Decision Process</strong>. Most common method for representing a reinforcement problem. MDPs consist of states, actions, and rewards. Total reward is sum of current (includes previous) and <em>discounted</em> future rewards: \[ R_t = r_t \gamma(r_{t + 1} + \gamma(r_{t + 2} + \ldots)) = r_t + \gamma R_{t + 1} \]</p> <p><strong>Q - learning</strong>. Define function \(Q(s, a)\) to be best possible score at end of game after performing action \(a\) in state \(s\); the “quality” of an action from a given state. The recursive definition of Q (for one transition) is given below in the <em>Bellman equation</em>.</p> <p>\[ Q(s, a) = r + \gamma \mathrm{max}_{a’} Q(s’, a’) \]</p> <p>and updates are computed with a learning rate \(\alpha\)as</p> <p>\[ Q(s_t, a_t) = (1 - \alpha)\cdot Q(s_{t -1}, a_{t - 1}) + \alpha \cdot (r + \gamma \max_{a’} Q(s’<em>{t + 1}, a</em>{t+1}’) ) \]</p> <p><strong>Deep Q Network</strong>. Deep learning can take deal with issues related to prohibitively large state spaces. The implementation chosen by DeepMind was to represent the Q-function with a neural network, with the states (pixels) as the input and Q-values as output, where the number of output neurons is the number of possible actions from the input state. We can optimize with simple squared loss:</p> <p><img src="http://localhost:4000/assets/img/drl/DRL_loss.PNG" style="width: 250px;"></p> <p>and our algorithm from some state $s$ becomes</p> <ol> <li> <p><strong>First forward pass</strong> from $s$ to get all predicted Q-values for each possible action. Choose action corresponding to max output, leading to next $s’$.</p> </li> <li> <p><strong>Second forward pass</strong> from $s’$ and again compute $\max_{a’} Q(s’, a’)$.</p> </li> <li> <p><strong>Set target output</strong> for each action $a’$ from $s’$. For the action corresponding to max (from step 2) set its target as $r + \gamma \max_{a’} Q(s’, a’)$, and for all other actions set target to same as originally returned from step 1, making the error 0 for those outputs. (Interpret as update to our guess for the best Q-value, and keep the others the same.)</p> </li> <li> <p><strong>Update weights</strong> using backprop.</p> </li> </ol> <p><strong>Experience Replay</strong>. This the most important trick for helping convergence of Q-values when approximating with non-linear functions. During gameplay all the experience $&lt;s, a, r, s’&gt;$ are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition.</p> <p><strong>Exploration</strong>. One could say that initializing the Q-values randomly and then picking the max is essentially a form of exploitation. However, this type of exploration is *greedy}, which can be tamed/fixed with \textbf{$\bm{\varepsilon}$-greedy exploration}. This incorporates a degree of randomness when choosing next action at *all} time-steps, determined by probability $\varepsilon$ that we choose the next action randomly. For example, DeepMind decreases $\varepsilon$ over time from 1 to 0.1.</p> <p><strong>Deep Q-Learning Algorithm</strong>.</p> <p><img src="http://localhost:4000/assets/img/drl/DRL_alg.PNG" style="width: 400px;"></p> <div class="entry-meta"> <br> <hr> <span class="entry-tags"><a href="http://localhost:4000/tags/#condensed%20tutorial" title="Pages tagged condensed tutorial" class="tag"><span class="term">condensed tutorial</span></a><a href="http://localhost:4000/tags/#deep%20learning" title="Pages tagged deep learning" class="tag"><span class="term">deep learning</span></a><a href="http://localhost:4000/tags/#reinforcement%20learning" title="Pages tagged reinforcement learning" class="tag"><span class="term">reinforcement learning</span></a></span> <span class="social-share"> <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/drl/" title="Share on Facebook" class="tag"> <span class="term"><i class="fa fa-facebook-square"></i> Like</span> </a> <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/drl/" title="Share on Twitter" class="tag"> <span class="term"><i class="fa fa-twitter-square"></i> Tweet</span> </a> <a href="https://plus.google.com/share?url=http://localhost:4000/drl/" title="Share on Google+" class="tag"> <span class="term"><i class="fa fa-google-plus-square"></i> +1</span> </a> </span> <div style="clear:both"></div> </div> </div> </div> </header> <!-- JS --> <script src="http://localhost:4000/assets/js/jquery-1.12.0.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.dlmenu.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.goup.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.magnific-popup.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.fitvid.min.js"></script> <script src="http://localhost:4000/assets/js/scripts.js"></script> <!-- MathJax --> <script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </body> </html>
