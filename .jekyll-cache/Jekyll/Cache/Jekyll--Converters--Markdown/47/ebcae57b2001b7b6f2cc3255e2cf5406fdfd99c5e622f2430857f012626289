I"{<h1 id="supervised-learning">Supervised Learning</h1>

<h2 id="linear-regression">Linear Regression</h2>

<p>Given feature vector \(\mathbf{x} \in \mathbb{R}^d\) and parameters \(\mathbf{\beta} \in \mathbb{R}^d\), predicts scalar \(y \in \mathbb{R}\) via:</p>

<p>\[y = \mathbf{\beta}^T\mathbf{x}\]</p>

<h2 id="logistic-regression">Logistic Regression</h2>

<p>Essentially an extension of linear regression for classification (discrete labels). Let \(K\) denote the number of classes.</p>

<p>\[\begin{align}
    y &amp;= arg\max_k Pr(k \mid x)<br />
    \\ 
    Pr(k \mid x) &amp;= \frac{\exp{\mathbf{\beta}_k^T\mathbf{x}}}{1+\sum_{\ell=1}^{K-1} \exp{\mathbf{\beta}_{\ell}^T\mathbf{x}}}  \quad (1 \le k \le K -1)<br />
    \\ 
    Pr(K \mid x) &amp;= \frac{1}{1+\sum_{\ell=1}^{K-1} \exp{\mathbf{\beta}_{\ell}^T\mathbf{x}}} 
\end{align}\]</p>

<h2 id="support-vector-machine-svm">Support Vector Machine (SVM)</h2>

<p>Extension of the <em>support vector classifier</em> (SVC) in order to accomodate non-linear class boundaries. The SVC is itself an extension of the <em>maximum margin classifier</em> that drops the constraint that all classes are linearly separable. SVMs are intended for binary classification, but they can be extended for the multi-class case.</p>

<p>In \(\mathbb{R}^n\), a <strong>hyperplane</strong> is a flat affine subspace of dimension \(n-1\). It is defined as the set of points \(\mathbf{x} \in \mathbb{R}^n\) for which
\[ \beta_0 + \mathbf{\beta}^T \mathbf{x} = 0\]</p>

<p>When \(\beta_0 = 0\) (i.e. the hyperplane goes through the origin), this is all vectors \(\mathbf{x}\) that are orthogonal to \(\mathbf{\beta}\). (TODO: interpretation/formula of normal vector to plane when \(\beta_0 \ne 0\)).</p>

<p>A key property of a hyperplane is that it defines the space in two. Suppose, instead of having binary labels 0 and 1 like we did in logistic regression, our labels are -1 and +1. Also suppose that all of our labeled data points \(\mathbf{x}\) can be perfectly separated by a hyperplane, such that all points with label -1 are on one side and all points with label +1 are on the other side. We can state this formally as, for any labeled example \((\mathbf{x}_i, y_i)\)
\[y_i(\beta_0 + \mathbf{\beta}^T\mathbf{x}_i) &gt; 0\]</p>

<p>In general, if our data is linearly separable, many such hyperplanes will exist. The <em>maximum margin classifier</em> tries to find the separating hyperplane that is farthest from the training observations. The <em>margin</em> is defined as the smallest distance from the hyperplane to a training data point. For any input \(\mathbf{x}\), the MMC predicts \(y=sign(\beta_0 + \mathbf{\beta}^T\mathbf{x})\)</p>

<ul>
  <li>KNN</li>
  <li>Linear Discriminant Analysis (LDA) (generative)</li>
  <li>Quadratic Discriminant Analysis (QDA) (generative)</li>
  <li>Neural Networks</li>
  <li>Decision Trees / Random Forests / boosting / etc</li>
  <li>Perceptron</li>
  <li>Naive Bayes</li>
</ul>

<h1 id="unsupervised-learning">Unsupervised Learning</h1>

<ul>
  <li>K-Means</li>
  <li>PCA</li>
  <li>SVD / matrix factorization</li>
</ul>

<p>Less must-know but still important:</p>
<ul>
  <li>Hierarchical Clustering</li>
  <li>Spectral Clustering</li>
</ul>

<h1 id="reinforcement-learning">Reinforcement Learning</h1>

<ul>
  <li>MDPs</li>
  <li>Games</li>
</ul>

<h1 id="generative-models">Generative Models</h1>

<ul>
  <li>Autoregressive
    <ul>
      <li>Masked Autoencoder for Distribution Estimation (MADE)</li>
    </ul>
  </li>
  <li>Latent Variable Models
    <ul>
      <li>Mixture of Gaussians (MoG)</li>
      <li>Variational Autoencoder (VAE)</li>
    </ul>
  </li>
  <li>Normalizing Flow Models
    <ul>
      <li>Nonlinear Independent Components Estimation (NICE)</li>
      <li>Real-NVP</li>
      <li>Masked Autoregressive Flow (MAF)</li>
      <li>Inverse Autoregressive Flow (IAF)</li>
    </ul>
  </li>
  <li>GANs</li>
  <li>Energy-Based Models</li>
  <li>Linear Discriminant Analysis</li>
  <li>Latent Dirichlet Allocation</li>
</ul>

<h1 id="graphical-models">Graphical Models</h1>

<ul>
  <li>Bayesian Networks</li>
  <li>Undirected Graphical Models</li>
  <li>Hidden Markov Models</li>
</ul>

<h1 id="feature-stuff">Feature Stuff?</h1>
<p>where should this even go</p>

<ul>
  <li>Kernel stuff</li>
  <li>Splines</li>
</ul>

<h1 id="other-todo">Other (TODO)</h1>

<ul>
  <li>CSPs</li>
</ul>

<h1 id="review-of-where-stuff-overlaps">Review of Where Stuff Overlaps</h1>

<p>TODO</p>
:ET